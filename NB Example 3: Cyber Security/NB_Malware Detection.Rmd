---
title: "Malware Classification"
graphics: yes
output: 
  pdf_document: 
    keep_tex: true
---

This code snippet shows you how to train a Naive Bayes (NB) algorithm for malware classification. For this purpose, we can use different features to train our algorithm. This includes features that are obtained using both static code analysis and/or analysis of the dynamic behavior of the malware.

**The dataset:** We will use the dataset published at https://data.mendeley.com/datasets/w2w8gjsgnt/1 (http://dx.doi.org/10.17632/w2w8gjsgnt.1#file-6806d890-e13f-4644-abc2-630cca78216f). This data set contains a total of 1944 features that are obtained from the static and dynamic analysis of ~ 19400 malware samples including malware samples from APT attacks. We pre-processed the dataset by removing the first three columns and the last seven columns. Instead, we added a label column at the end of the dataset.

**Machine learning (ML) task:** We want to create an NB model to identify (classify) the type of malware, so we are solving a classification problem here.

```{r message=FALSE}
set.seed(1234)
myDataOrg <- read.csv("malware.csv", header=T)
dim(myDataOrg) # check dimensions of myData
levels(myDataOrg$label) # check different levels (values) for each class
```

**Check the balance of the dataset** : Class imbalance is a very common problem in cyber security datasets, and it is quite common that a 1:100000 ratio between classes (e.g. attack: normal) due to the scarcity of attack data. If the class imbalance occurred then it can be affected on model performance. We tried an NB model for the whole datset and accuracy was ~34%. Let's look at ratio between classes in our dataset.

```{r message=FALSE}
print(table(myDataOrg$label))
```

As you can see, our dataset is a hugely imbalance, especially Backdoor: Trojan. A mix of oversampling and undersampling methods could be utilised to balance the dataset, e.g., by increasing the size of Backdoor class and reducing the size of Trojan class. However, this can be resulted information lost in the larger class. Therefore, we will split the data set into two subsets and train two NB models separately (Ensamble technique). Of course, if NB doesn't work very well with one dataset, you can train another model (e.g. random forest) for that data set and combine NB with other model for better performance.

**Note**: It should be noted that NB woud not be the best option for this type of dataset, instead we recommend to try out some ensamble techniques. However, we will train NB model in this way as our goal in this post to show you how to train the NB model for this dataset.

```{r}
myDataSet1<-rbind(subset(myDataOrg, label == "Backdoor"),subset(myDataOrg, label == "OtherType"),subset(myDataOrg, label == "Rootkit"),subset(myDataOrg, label == "Spyware"))

myDataSet2<-rbind(subset(myDataOrg, label == "Trojan"),subset(myDataOrg, label == "Unknown"),subset(myDataOrg, label == "Worm"))

```

We split the original datset into two subsets. Let's continue with myDataSet1. You can follow the same approach for myDataSet2 or fit a different model as mentioned above. 

The following code makes class sizes equal in myDataSet1.

```{r message=FALSE}
sample.df <- function(df, n) df[sample(nrow(df), n,replace = T), , drop = F]

classSize<-500 # sample from each class size of 500 records

myData<-rbind(sample.df(subset(myDataSet1, label == "Backdoor"), classSize),sample.df(subset(myDataSet1, label == "OtherType"), classSize),sample.df(subset(myDataSet1, label == "Rootkit"), classSize),sample.df(subset(myDataSet1, label == "Spyware"), classSize))

```

Since we want to represent the presence or absence of a certain static/dynamic feature in the malware, we code our dataset as follows.

```{r message=FALSE}
convert_counts <- function(x) {
  x <- ifelse(x > 0, "Y", "N")
}
myData <- data.frame(apply(myData[,1:1934], MARGIN = 2,convert_counts),myData[1935] )

```

**Creating training and validation datasets:** We’re going to follow the convention of 80/20 samples ratio to partition the dataset to the training and validation sets. We use the createDataPartition function from the caret package for this purpose.

```{r message=FALSE}
#install.packages("caret") #If the caret package is not installed on your system, uncomment this line to install it first
set.seed(1234)
library(caret) #Loading the library
tr_index <- createDataPartition(myData$label, p=0.80, list=FALSE) # List of 80% of the rows
trainSet <- myData[tr_index,] # select 80% of the data for the trainSet
testSet <- myData[-tr_index,] # Select the remaining 20% of data for testSet
```

**Building a NB classifier:** Now we will train our NB classiﬁer using the above trainSet. For this purpose, we will utilize e1071 package in R. Note that the priori probabilities can be computed using the following lines of code. In this case, the 
priori probabilities for all classes are the same. 

```{r message=FALSE}
#install.packages("e1071") #If the e1071 package is not installed on your system, uncomment this line to install it first
library(e1071)
NBclassfier <- naiveBayes(trainSet[,1:1934], trainSet$label) # train the model

```

**Make predictions:** Now let's apply the above model to assign labels for test cases in testSet. Then we create the confusion matrix, a table that is often used to describe the performance of a classifier.

```{r message=FALSE}
  testPrediction <- predict(NBclassfier, testSet[,1:1934]) # predict labels for test cases
  confusionMatrix(testPrediction, testSet$label) # Print confusion matrix 
```
